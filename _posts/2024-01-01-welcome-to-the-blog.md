---
layout: single
title: "Welcome to the Blog"
date: 2025-09-28 00:00:00 -0700
categories: [journal]
tags: [introduction]
excerpt: "A quick introduction to what this blog is about."
---

After recent interviews with OpenAI, xAI, and Anthropic, I realized I had been struggling in the technical rounds. Relying on AI copilots in my projects and research has not helped me refresh my fundamental knowledge. This blog is my way of rebuilding my foundation as both an engineer and a researcher.

## Purpose  

I plan to strengthen both my conceptual understanding and coding fluency by systematically reviewing common components of modern LLMs and their practical implementations.  

## Topics Iâ€™ll Cover (Tentative)

- **Core components of LLMs**: from tokenizers and the basic GPT-2 style decoder transformer architecture, to more recent modifications such as MLA, RoPE, and MoE.  
- **Systems-level optimizations**: techniques like KV-cache, speculative decoding, and tensor parallelism.  

## What to Expect  

- High-level technical explanations of concepts, without going into full mathematical derivations if the topic is too theoretical.  
- PyTorch-based implementation, relying on common libraries unless a lower-level implementation is much more beneficial.
- Occasional reflections on what worked, along with possible research questions that arise from the topics.  

My aim is to update the blog twice per week, with each post representing roughly 3 hours of focused, non-LLM assisted work.  